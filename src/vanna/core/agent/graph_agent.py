"""
GraphAgent ä½¿ç”¨ LangGraph çš„å®ç°ï¼Œé€‚ç”¨äº Vanna Agents æ¡†æ¶ã€‚

è¯¥æ¨¡å—æä¾›äº† GraphAgent ç±»ï¼Œå®ƒé€šè¿‡çŠ¶æ€å›¾åœ¨ LLM æœåŠ¡ã€å·¥å…·å’Œä¼šè¯å­˜å‚¨ä¹‹é—´è¿›è¡Œç¼–æ’ä¸åä½œã€‚
"""

import asyncio
import logging
import traceback
import uuid
from typing import Any, AsyncGenerator, Dict, List, Literal, Optional, TypedDict, Union

from langgraph.graph import END, StateGraph
import functools


from vanna.components import (
    ChatInputUpdateComponent,
    RichTextComponent,
    SimpleTextComponent,
    StatusBarUpdateComponent,
    StatusCardComponent,
    CardComponent,  # Add CardComponent
    Task,
    TaskTrackerUpdateComponent,
    UiComponent,
)
from vanna.capabilities.agent_memory import AgentMemory
from vanna.core.agent.config import AgentConfig, UiFeature
from vanna.core.audit import AuditLogger
from vanna.core.enricher import ToolContextEnricher
from vanna.core.enhancer import LlmContextEnhancer, DefaultLlmContextEnhancer
from vanna.core.filter import ConversationFilter
from vanna.core.lifecycle import LifecycleHook
from vanna.core.llm import LlmMessage, LlmRequest, LlmResponse, LlmService
from vanna.core.middleware import LlmMiddleware
from vanna.core.observability import ObservabilityProvider
from vanna.core.recovery import ErrorRecoveryStrategy
from vanna.core.registry import ToolRegistry
from vanna.core.storage import Conversation, ConversationStore, Message
from vanna.core.system_prompt import DefaultSystemPromptBuilder, SystemPromptBuilder
from vanna.core.tool import ToolContext, ToolSchema
from vanna.core.user import User
from vanna.core.user.request_context import RequestContext
from vanna.core.user.resolver import UserResolver
from vanna.core.workflow import DefaultWorkflowHandler, WorkflowHandler

logger = logging.getLogger(__name__)


class AgentState(TypedDict):
    """Agent æ‰§è¡ŒçŠ¶æ€å›¾çš„çŠ¶æ€å®šä¹‰ã€‚"""

    # ä¸Šä¸‹æ–‡
    request_context: RequestContext
    user: User
    conversation_id: str
    request_id: str
    conversation: Conversation
    agent_memory: AgentMemory
    observability_provider: Optional[ObservabilityProvider]

    # è¾“å…¥
    message: str

    # å·¥ä½œæµæ•°æ®
    ui_queue: asyncio.Queue  # ç”¨äºæµå¼å‘é€ UI ç»„ä»¶çš„é˜Ÿåˆ—
    is_starter_request: bool
    should_stop: bool

    # LLM äº¤äº’
    tool_schemas: List[ToolSchema]
    system_prompt: Optional[str]
    messages: List[LlmMessage]
    llm_request: Optional[LlmRequest]
    llm_response: Optional[LlmResponse]

    # æ‰§è¡Œæ§åˆ¶
    tool_iterations: int
    tool_iterations: int
    tool_context: Optional[ToolContext]

    # æ¨¡å‹ä¸ SQL
    schema_metadata: Optional[str]
    generated_sql: Optional[str]
    sql_result: Optional[str]


# å±€éƒ¨çŠ¶æ€æ›´æ–°çš„è¾…åŠ©ç±»å‹
PartialAgentState = Dict[str, Any]


class GraphAgent:
    """
    ä½¿ç”¨ LangGraph è¿›è¡Œå®è§‚ç¼–æ’çš„ Agent å®ç°ã€‚

    è¯¥ç±»åœ¨ä¿æŒä¸æ ‡å‡† Agent ç±» API å…¼å®¹çš„åŒæ—¶ï¼Œ
    ä½¿ç”¨æœ‰å‘å¾ªç¯å›¾å®ç°å†…éƒ¨çš„å†³ç­–ä¸æ§åˆ¶å›è·¯ã€‚
    """

    def __init__(
        self,
        llm_service: LlmService,
        tool_registry: ToolRegistry,
        user_resolver: UserResolver,
        agent_memory: AgentMemory,
        conversation_store: Optional[ConversationStore] = None,
        config: AgentConfig = AgentConfig(),
        system_prompt_builder: SystemPromptBuilder = DefaultSystemPromptBuilder(),
        lifecycle_hooks: List[LifecycleHook] = [],
        llm_middlewares: List[LlmMiddleware] = [],
        workflow_handler: Optional[WorkflowHandler] = None,
        error_recovery_strategy: Optional[ErrorRecoveryStrategy] = None,
        context_enrichers: List[ToolContextEnricher] = [],
        llm_context_enhancer: Optional[LlmContextEnhancer] = None,
        conversation_filters: List[ConversationFilter] = [],
        observability_provider: Optional[ObservabilityProvider] = None,
        audit_logger: Optional[AuditLogger] = None,
    ):
        self.llm_service = llm_service
        self.tool_registry = tool_registry
        self.user_resolver = user_resolver
        self.agent_memory = agent_memory

        if conversation_store is None:
            from vanna.integrations.local import MemoryConversationStore
            conversation_store = MemoryConversationStore()

        self.conversation_store = conversation_store
        self.config = config
        self.system_prompt_builder = system_prompt_builder
        self.lifecycle_hooks = lifecycle_hooks
        self.llm_middlewares = llm_middlewares

        if workflow_handler is None:
            workflow_handler = DefaultWorkflowHandler()
        self.workflow_handler = workflow_handler

        self.error_recovery_strategy = error_recovery_strategy
        self.context_enrichers = context_enrichers

        if llm_context_enhancer is None:
            llm_context_enhancer = DefaultLlmContextEnhancer(agent_memory)
        self.llm_context_enhancer = llm_context_enhancer

        self.conversation_filters = conversation_filters
        self.observability_provider = observability_provider
        self.audit_logger = audit_logger

        if self.audit_logger and self.config.audit_config.enabled:
            self.tool_registry.audit_logger = self.audit_logger
            self.tool_registry.audit_config = self.config.audit_config

        # åˆå§‹åŒ–çŠ¶æ€å›¾
        self.graph = self._build_graph()
        logger.info(f"Graph: {self.graph.get_graph().draw_mermaid()}")
        logger.info("Initialized GraphAgent")

    def _build_graph(self) -> Any:
        """æ„å»º LangGraph çŠ¶æ€æœºã€‚"""
        workflow = StateGraph(AgentState)

        # ç›´æ¥æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("initialize", self._node_initialize)
        workflow.add_node("memory_search", self._node_memory_search)  # æ–°å¢è®°å¿†æœç´¢èŠ‚ç‚¹
        workflow.add_node("get_schema", self._node_get_schema)
        workflow.add_node("think", self._node_think)
        workflow.add_node("generate_sql", self._node_generate_sql)
        workflow.add_node("execute_sql", self._node_execute_sql)
        workflow.add_node("save_memory", self._node_save_memory)  # æ–°å¢è®°å¿†ä¿å­˜èŠ‚ç‚¹
        workflow.add_node("execute_tools", self._node_execute_tools)
        workflow.add_node("finalize", self._node_finalize)

        workflow.set_entry_point("initialize")

        # åˆå§‹åŒ–é˜¶æ®µä¼šå‡†å¤‡ä¸Šä¸‹æ–‡ï¼Œå› æ­¤å¦‚æœæ˜¯èµ·å§‹è¯·æ±‚å¯ç›´æ¥å¾ªç¯åˆ°æ€è€ƒæˆ–ç‰¹å®šèŠ‚ç‚¹ï¼›
        # å®é™…ä¸Šåˆå§‹åŒ–ä¼šè¿”å›ç”¨æˆ·ä¸ä¸Šä¸‹æ–‡ï¼Œæ‰€ä»¥ä¸‹ä¸€æ­¥ä¸ºæ€è€ƒèŠ‚ç‚¹

        workflow.add_conditional_edges(
            "initialize",
            self._router_check_stop,
            {
                "stop": "finalize",
                "continue": "memory_search"  # åˆå§‹åŒ–åå…ˆæœç´¢è®°å¿†
            }
        )
        
        # è®°å¿†æœç´¢åè¿›å…¥æ€è€ƒèŠ‚ç‚¹
        workflow.add_edge("memory_search", "think")

        # æ‰€æœ‰åŠ¨ä½œèŠ‚ç‚¹æœ€ç»ˆå›åˆ°æ€è€ƒèŠ‚ç‚¹
        workflow.add_edge("get_schema", "think")
        workflow.add_edge("generate_sql", "think")
        
        # æ‰§è¡ŒSQLæˆåŠŸåå°è¯•ä¿å­˜è®°å¿†ï¼Œç„¶åå›æ€è€ƒ
        workflow.add_edge("execute_sql", "save_memory")
        workflow.add_edge("save_memory", "think")

        # ä»æ€è€ƒèŠ‚ç‚¹çš„æ¡ä»¶è¾¹ï¼ˆå·¥å…·æ‰§è¡Œ / å®Œæˆ / è™šæ‹Ÿå·¥å…·ï¼‰
        workflow.add_conditional_edges(
            "think",
            self._router_analyze_response,
            {
                "tools": "execute_tools",
                "done": "finalize",
                "get_schema": "get_schema",
                "generate_sql": "generate_sql",
                "execute_sql": "execute_sql"
            }
        )

        # ä»å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹å›åˆ°æ€è€ƒèŠ‚ç‚¹
        workflow.add_conditional_edges(
            "execute_tools",
            self._router_check_limit,
            {
                "continue": "think",
                "stop": "finalize"
            }
        )

        workflow.add_edge("finalize", END)

        return workflow.compile()

    def _sanitize_messages_for_llm(self, messages: List[LlmMessage]) -> List[LlmMessage]:
        """Ensure assistant messages with tool_calls are followed by tool responses.

        Some providers (OpenAI-compatible) require that any assistant message
        containing tool_calls MUST be immediately followed by tool messages
        responding to each tool_call_id. If history contains an assistant
        tool_calls message without corresponding tool responses (e.g., from
        an interrupted prior run), we drop the tool_calls to avoid protocol
        errors while preserving any textual content.
        """
        sanitized: List[LlmMessage] = []
        pending_tool_ids: List[str] = []

        for i, msg in enumerate(messages):
            if msg.role == "assistant" and msg.tool_calls:
                # Collect tool_call_ids
                pending_tool_ids = [tc.id for tc in (msg.tool_calls or [])]

                # Look ahead for tool responses
                has_responses = True
                ids_remaining = set(pending_tool_ids)
                for j in range(i + 1, len(messages)):
                    nxt = messages[j]
                    if nxt.role != "tool":
                        # Encountered a non-tool message before all responses
                        break
                    if nxt.tool_call_id:
                        ids_remaining.discard(nxt.tool_call_id)
                    if not ids_remaining:
                        has_responses = True
                        break

                # If responses missing, strip tool_calls to satisfy protocol
                if ids_remaining:
                    sanitized.append(LlmMessage(role="assistant", content=msg.content or ""))
                else:
                    sanitized.append(msg)
            else:
                sanitized.append(msg)

        return sanitized

    async def send_message(
        self,
        request_context: RequestContext,
        message: str,
        *,
        conversation_id: Optional[str] = None,
    ) -> AsyncGenerator[UiComponent, None]:
        """
        ä½¿ç”¨å›¾å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œå¹¶æŒ‰éœ€äº§ç”Ÿ UI ç»„ä»¶ã€‚
        """
        ui_queue = asyncio.Queue()

        # åˆå§‹çŠ¶æ€
        initial_state: AgentState = {
            "request_context": request_context,
            "user": None,  # Resolved in initialize
            "conversation_id": conversation_id,
            "request_id": str(uuid.uuid4()),
            "conversation": None,  # Loaded in initialize
            "agent_memory": self.agent_memory,
            "observability_provider": self.observability_provider,
            "message": message,
            "ui_queue": ui_queue,
            "is_starter_request": False,
            "should_stop": False,
            "tool_schemas": [],
            "system_prompt": None,
            "messages": [],
            "llm_request": None,
            "llm_response": None,
            "tool_iterations": 0,
            "tool_context": None,
            "schema_metadata": None,
            "generated_sql": None,
            "sql_result": None,
        }

        # ä½¿ç”¨ stream_mode="updates" è·å–èŠ‚ç‚¹æ‰§è¡Œçš„å¢é‡æ›´æ–°
        try:
            # Create a task to run the graph
            async def run_graph():
                try:
                    async for event in self.graph.astream(initial_state, stream_mode="updates"):
                        # event æ˜¯ä¸€ä¸ªå­—å…¸ï¼š{node_name: node_output}
                        for node_name, node_output in event.items():
                            # node_output æ˜¯èŠ‚ç‚¹è¿”å›çš„çŠ¶æ€æ›´æ–°ï¼ˆå±€éƒ¨çŠ¶æ€ï¼‰
                            if isinstance(node_output, dict):
                                # æ‰“å°é”®å
                                logger.info(f"Node '{node_name}' updated: {list(node_output.keys())}")
                                # æ‰“å°æ¯ä¸ªé”®çš„å€¼ï¼ˆé™åˆ¶é•¿åº¦é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                                for key, value in node_output.items():
                                    value_str = str(value)
                                    logger.info(f"  {key}: {value_str}")
                            else:
                                logger.info(f"Node '{node_name}' updated: completed")
                            logger.info("====================")
                except Exception as e:
                    logger.error(f"Error in graph execution: {e}", exc_info=True)
                    await ui_queue.put(e)
                finally:
                    # Signal completion
                    await ui_queue.put(None)

            # Start the graph task
            graph_task = asyncio.create_task(run_graph())

            # Consume the UI queue while the graph runs
            while True:
                item = await ui_queue.get()
                
                if item is None:
                    # Completion signal
                    break
                
                if isinstance(item, Exception):
                    # Error occurred in graph
                    yield UiComponent(
                        rich_component=StatusCardComponent(
                            title="Error Processing Message",
                            status="error",
                            description="An unexpected error occurred.",
                            icon="âš ï¸",
                        ),
                        simple_component=SimpleTextComponent(text=f"Error: {str(item)}")
                    )
                    break

                yield item

            # Ensure graph task is done
            await graph_task

        except Exception as e:
            # é”™è¯¯å¤„ç†æ–¹å¼ä¸ä¼ ç»Ÿ Agent ç±»ä¸€è‡´
            logger.error(f"Error in GraphAgent: {e}", exc_info=True)
            yield UiComponent(
                rich_component=StatusCardComponent(
                    title="Error Processing Message",
                    status="error",
                    description="An unexpected error occurred.",
                    icon="âš ï¸",
                ),
                simple_component=SimpleTextComponent(text=f"Error: {str(e)}")
            )

    # --- èŠ‚ç‚¹å®ç° ---

    async def _node_initialize(self, state: AgentState) -> PartialAgentState:
        """
        åˆå¹¶çš„åˆå§‹åŒ–èŠ‚ç‚¹ï¼š
        1. è§£æç”¨æˆ·ä¸ä¼šè¯ï¼›
        2. å¤„ç†å·¥ä½œæµ/èµ·å§‹ç•Œé¢ï¼›
        3. å‡†å¤‡å·¥å…·ä¸Šä¸‹æ–‡ä¸ç³»ç»Ÿæç¤ºè¯ã€‚
        """
        request_context = state["request_context"]
        message = state["message"]
        conversation_id = state["conversation_id"]
        ui_queue = state["ui_queue"]

        # 1. è§£æç”¨æˆ·
        user = await self.user_resolver.resolve_user(request_context)

        # 2. æ£€æŸ¥æ˜¯å¦ä¸ºèµ·å§‹è¯·æ±‚ / å·¥ä½œæµ
        is_starter_request = (not message.strip()) or request_context.metadata.get(
            "starter_ui_request", False
        )

        if is_starter_request and self.workflow_handler:
            if conversation_id is None:
                conversation_id = str(uuid.uuid4())
            conversation = await self.conversation_store.get_conversation(conversation_id, user)
            if not conversation:
                conversation = Conversation(id=conversation_id, user=user, messages=[])

            components = await self.workflow_handler.get_starter_ui(self, user, conversation)
            if components:
                for comp in components:
                    await ui_queue.put(comp)
                await ui_queue.put(UiComponent(rich_component=StatusBarUpdateComponent(status="idle", message="Ready", detail="Choose an option")))
                await ui_queue.put(UiComponent(rich_component=ChatInputUpdateComponent(placeholder="Ask a question...", disabled=False)))
                if self.config.auto_save_conversations:
                    await self.conversation_store.update_conversation(conversation)
                return {
                    "user": user,
                    "conversation": conversation,
                    "conversation_id": conversation_id,
                    "is_starter_request": True,
                    "should_stop": True
                }

        if not message.strip():
            return {"user": user, "should_stop": True}

        # ç”Ÿå‘½å‘¨æœŸé’©å­ï¼šæ¶ˆæ¯å‰ç½®å¤„ç†
        for hook in self.lifecycle_hooks:
            result = await hook.before_message(user, message)
            if result is not None:
                message = result

        if conversation_id is None:
            conversation_id = str(uuid.uuid4())

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working", message="Processing...", detail="Initializing context"
            )
        ))

        conversation = await self.conversation_store.get_conversation(conversation_id, user)
        is_new = False
        if not conversation:
            conversation = Conversation(id=conversation_id, user=user, messages=[])
            is_new = True

        if is_new:
            await self.conversation_store.update_conversation(conversation)

        conversation.add_message(Message(role="user", content=message))

        # 3. å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆåŸ _node_prepare_contextï¼‰
        context_task = Task(title="Load context", status="pending")
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.add_task(context_task)))

        # æ„å»ºå·¥å…·ä¸Šä¸‹æ–‡
        ui_features_available = []
        for feature_name in self.config.ui_features.feature_group_access.keys():
            if self.config.ui_features.can_user_access_feature(feature_name, user):
                ui_features_available.append(feature_name)

        tool_context = ToolContext(
            user=user,
            conversation_id=conversation.id,
            request_id=state["request_id"],
            agent_memory=self.agent_memory,
            observability_provider=self.observability_provider,
            metadata={"ui_features_available": ui_features_available},
        )

        for enricher in self.context_enrichers:
            tool_context = await enricher.enrich_context(tool_context)

        # è·å–å¯ç”¨å·¥å…·
        tool_schemas = await self.tool_registry.get_schemas(user)

        await ui_queue.put(UiComponent(
            rich_component=TaskTrackerUpdateComponent.update_task(
                context_task.id, status="completed"
            )
        ))

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = await self.system_prompt_builder.build_system_prompt(
            user, tool_schemas
        )
        if self.llm_context_enhancer and system_prompt:
            system_prompt = await self.llm_context_enhancer.enhance_system_prompt(
                system_prompt, message, user
            )

        # è¿‡æ»¤ä¼šè¯æ¶ˆæ¯
        filtered_messages = conversation.messages
        for filter in self.conversation_filters:
            filtered_messages = await filter.filter_messages(filtered_messages)

        # è½¬æ¢ä¸º LlmMessage åˆ—è¡¨
        messages = [
            LlmMessage(
                role=msg.role,
                content=msg.content,
                tool_calls=msg.tool_calls,
                tool_call_id=msg.tool_call_id
            )
            for msg in filtered_messages
        ]

        if self.llm_context_enhancer:
            messages = await self.llm_context_enhancer.enhance_user_messages(messages, user)

        # Sanitize history to avoid unresolved tool_calls protocol errors
        messages = self._sanitize_messages_for_llm(messages)

        return {
            "user": user,
            "conversation": conversation,
            "conversation_id": conversation_id,
            "message": message,
            "tool_context": tool_context,
            "tool_schemas": tool_schemas,
            "system_prompt": system_prompt,
            "messages": messages,
            "should_stop": False
        }

    async def _node_memory_search(self, state: AgentState) -> PartialAgentState:
        """
        è®°å¿†æœç´¢èŠ‚ç‚¹ï¼š
        ä½¿ç”¨ search_saved_correct_tool_uses å·¥å…·æ£€ç´¢ç›¸ä¼¼çš„å†å²æ“ä½œã€‚
        """
        ui_queue = state["ui_queue"]
        context = state["tool_context"]
        message = state["message"]
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è®°å¿†åŠŸèƒ½
        if not self.agent_memory:
            return {}

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†è®°å¿†æœç´¢å·¥å…·
        search_tool = await self.tool_registry.get_tool("search_saved_correct_tool_uses")
        if not search_tool:
            return {}

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working", message="Searching Memory", detail="Checking past experiences..."
            )
        ))
        
        # æ·»åŠ ä»»åŠ¡
        task = Task(title="Search Memory", description="Searching for similar past queries", status="in_progress")
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.add_task(task)))

        try:
            # æ‰§è¡Œæœç´¢
            args_model = search_tool.get_args_schema()
            # æœç´¢ç›¸ä¼¼é—®é¢˜ï¼Œåªå…³æ³¨ run_sql ç±»å‹çš„å·¥å…·è°ƒç”¨
            tool_args = args_model(question=message, tool_name_filter="run_sql")
            
            result = await search_tool.execute(context, tool_args)
            
            # å¦‚æœæ‰¾åˆ°äº†ç»“æœï¼Œå°†å…¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡æ¶ˆæ¯ä¸­
            if result.success and "Found" in result.result_for_llm and "0 similar" not in result.result_for_llm:
                memory_msg = f"Memory Search Results:\n{result.result_for_llm}"
                state["messages"].append(LlmMessage(role="system", content=memory_msg))
                
                await ui_queue.put(UiComponent(
                    rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="completed", detail="Found relevant memories")
                ))
            else:
                await ui_queue.put(UiComponent(
                    rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="completed", detail="No relevant memories found")
                ))
                
            if result.ui_component:
                await ui_queue.put(result.ui_component)
                
        except Exception as e:
            logger.error(f"Memory search failed: {e}")
            await ui_queue.put(UiComponent(
                rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="failed", detail=str(e))
            ))

        return {}

    async def _node_get_schema(self, state: AgentState) -> PartialAgentState:
        """
        ä¸»åŠ¨çš„æ¶æ„æ£€ç´¢èŠ‚ç‚¹ã€‚
        æ‰§è¡Œ LLM æä¾›çš„ SQLï¼ˆé€šè¿‡ query_schema_metadataï¼‰æ¥æ£€æŸ¥æ•°æ®åº“ç»“æ„ï¼Œ
        å°†ç»“æœä¿å­˜å¹¶åŠ å…¥ä¸Šä¸‹æ–‡ã€‚
        """
        # if state.get("should_stop"): return {} # æ— éœ€å¤„ç†ï¼Œè·¯ç”±å·²æ˜ç¡®ä¿éšœåˆ°è¾¾æ­¤å¤„

        context = state.get("tool_context")
        ui_queue = state.get("ui_queue")
        response = state.get("llm_response")

        # 1. ä»å·¥å…·è°ƒç”¨ä¸­è§£æ SQLï¼Œå¹¶å¤„ç†æ‰€æœ‰å·¥å…·è°ƒç”¨
        schema_sql = "SELECT name, sql FROM sqlite_master WHERE type='table'"  # å…œåº•å€¼

        target_tool_id = None
        other_tool_ids = []

        if response and response.tool_calls:
            for tc in response.tool_calls:
                if tc.name == "query_schema_metadata":
                    # ä» LLM æä¾›çš„å‚æ•°ä¸­è·å– SQLï¼Œè‹¥æ— åˆ™ä½¿ç”¨å…œåº•å€¼
                    provided_sql = tc.arguments.get("sql")
                    if provided_sql:
                        schema_sql = provided_sql
                    target_tool_id = tc.id
                else:
                    other_tool_ids.append(tc.id)

        if not schema_sql:
            # å¿…é¡»å¯¹å·¥å…·è°ƒç”¨è¿›è¡Œå“åº”
            for tc in (response.tool_calls or []):
                state["messages"].append(LlmMessage(
                    role="tool",
                    content="Error: No SQL argument provided for schema query.",
                    tool_call_id=tc.id
                ))
            return {}

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working", message="Querying Schema", detail="Inspecting database..."
            )
        ))

        # æ·»åŠ ä»»åŠ¡å’ŒçŠ¶æ€å¡ç‰‡
        task = Task(title="Query Schema", description="Inspecting database structure", status="in_progress")
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.add_task(task)))

        # 2. æ‰§è¡Œ SQLï¼ˆå¤ç”¨ run_sql å·¥å…·é€»è¾‘ï¼‰
        query_result_text = ""
        success = False
        try:
            sql_tool = await self.tool_registry.get_tool("run_sql")
            if not sql_tool:
                raise Exception("Refusing to query schema: 'run_sql' tool not available.")

            # æ‰§è¡Œ
            # æ‰‹åŠ¨æ„å»ºå‚æ•°
            args_model = sql_tool.get_args_schema()
            tool_args = args_model(sql=schema_sql)

            result = await sql_tool.execute(context, tool_args)
            success = result.success

            if result.success:
                # Check if we have actual data in metadata (for PRAGMA and other queries)
                # PRAGMA queries don't start with SELECT, so they return summary text in result_for_llm
                # but the actual data is available in metadata["results"]
                if result.metadata and "results" in result.metadata:
                    results = result.metadata["results"]
                    if results:
                        # Format the results as a readable structure for LLM
                        import json
                        query_result_text = result.result_for_llm + "\n\nData:\n" + json.dumps(results, indent=2, ensure_ascii=False)
                    else:
                        query_result_text = result.result_for_llm
                else:
                    query_result_text = result.result_for_llm
            else:
                query_result_text = f"Schema Query Failed: {result.error}"

        except Exception as e:
            logger.error(f"Schema Query Error: {e}")
            query_result_text = f"Schema Query Error: {e}"
            success = False

        # æ›´æ–°ä»»åŠ¡å’ŒçŠ¶æ€å¡ç‰‡
        # status = "success" if success else "error"
        # await ui_queue.put(UiComponent(rich_component=card.set_status(status, "Schema retrieved" if success else "Failed to retrieve schema")))
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="completed")))

        # 3. ä¿å­˜åˆ°è®°å¿†
        # ä½œä¸ºæ–‡æœ¬è®°å¿†ä¿å­˜ï¼Œåœ¨éœ€è¦æ—¶è·¨è½®æ¬¡/ä¼šè¯æŒä¹…åŒ–ï¼›
        # æˆ–åœ¨æœ¬æ¬¡ä¼šè¯ä¸­è¿½åŠ åˆ°ç³»ç»Ÿæç¤ºè¯ã€‚
        # ç”¨æˆ·éœ€æ±‚ï¼š"get_schemaè·å–å¹¶å­˜å‚¨åˆ°memoryé‡Œ"
        if self.agent_memory and "Error" not in query_result_text:
            # åˆ›å»ºä¸€æ¡æ–‡æœ¬è®°å¿†é¡¹
            # åœ¨å®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨ save_text_memory å·¥å…·é€»è¾‘æˆ–ç›´æ¥è°ƒç”¨ï¼›
            # æ­¤å¤„å‡è®¾å¯ç›´æ¥è®¿é—®ï¼Œè‹¥æ— ç›´æ¥ API åˆ™è·³è¿‡ï¼›
            # æš‚æ—¶å°†å…¶æ”¾å…¥ 'schema_metadata' çŠ¶æ€å­—æ®µï¼Œåç»­å¯æŒä¹…åŒ–ã€‚
            pass

        # 4. æ›´æ–°ä¸Šä¸‹æ–‡

        result_msg = f"Schema Query Result ({schema_sql}):\n{query_result_text}"

        # åŠ å…¥åˆ°æ¶ˆæ¯å†å²ä¸­ï¼Œä»¥ä¾¿ LLM æ„ŸçŸ¥
        if target_tool_id:
            state["messages"].append(LlmMessage(
                role="tool", content=result_msg, tool_call_id=target_tool_id))
        else:
            # å…œåº•ï¼šè‹¥æ‰¾ä¸åˆ° ID æˆ–éå·¥å…·è°ƒç”¨ï¼ˆåœ¨å½“å‰æµç¨‹ä¸­ä¸å¤ªå¯èƒ½ï¼‰
            state["messages"].append(LlmMessage(role="system", content=result_msg))

        # å¤„ç†å…¶ä»–å·¥å…·è°ƒç”¨ï¼ˆå ä½å“åº”ä»¥æ»¡è¶³ APIï¼‰
        for ot_id in other_tool_ids:
            state["messages"].append(LlmMessage(
                role="tool", content="Tool call ignored in this step.", tool_call_id=ot_id))

        return {
            "schema_metadata": query_result_text,  # Allow subsequent nodes to see it specifically
            "tool_iterations": state["tool_iterations"] + 1
        }

    async def _node_generate_sql(self, state: AgentState) -> PartialAgentState:
        """æ ¹æ®è¯·æ±‚ç”Ÿæˆ SQLã€‚"""
        ui_queue = state.get("ui_queue")
        response = state.get("llm_response")

        # è‹¥å·¥å…·è°ƒç”¨æä¾›äº†æŒ‡ä»¤åˆ™ä½¿ç”¨ï¼Œå¦åˆ™é‡‡ç”¨é€šç”¨æŒ‡ä»¤
        instruction = "Generate SQL for the user's request."
        if response and response.tool_calls:
            for tc in response.tool_calls:
                if tc.name == "generate_sql":
                    instruction = tc.arguments.get("instruction", instruction)
                    break

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working", message="Generating SQL", detail="Drafting query..."
            )
        ))

        # æ·»åŠ ä»»åŠ¡å’ŒçŠ¶æ€å¡ç‰‡
        task = Task(title="Generate SQL", description="Drafting SQL query", status="in_progress")
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.add_task(task)))

        # æŸ¥æ‰¾ generate_sql çš„å·¥å…·è°ƒç”¨ ID
        target_tool_id = None
        other_tool_ids = []
        if response and response.tool_calls:
            for tc in response.tool_calls:
                if tc.name == "generate_sql":
                    target_tool_id = tc.id
                else:
                    other_tool_ids.append(tc.id)

        if target_tool_id:
            # åœ¨å‘èµ·æ–°çš„è¯·æ±‚å‰ï¼Œå¿…é¡»å¯¹å·¥å…·è°ƒç”¨è¿›è¡Œé—­åˆå“åº”
            state["messages"].append(LlmMessage(
                role="tool", content="Proceeding with SQL generation.", tool_call_id=target_tool_id))

        # å¤„ç†å…¶ä»–å·¥å…·è°ƒç”¨
        for ot_id in other_tool_ids:
            state["messages"].append(LlmMessage(
                role="tool", content="Tool call ignored in this step.", tool_call_id=ot_id))

        # å°†ç‰¹å®šæŒ‡ä»¤é™„åŠ åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­ï¼›
        # æ³¨æ„ï¼šè‹¥åªå¸Œæœ› LLM è¾“å‡º SQLï¼Œå¯ä¸åŒ…å«åˆšæ·»åŠ çš„å·¥å…·å“åº”ï¼›
        # ä½†ä¸ºä¿è¯å†å²ä¸€è‡´æ€§ï¼Œé€šå¸¸åº”åŒ…å«ï¼›
        # å¯¹äºä¸“é—¨çš„â€œç”Ÿæˆâ€ä»»åŠ¡ï¼Œä¹Ÿå¯é€‚å½“å±è”½å†å²ï¼Œä»…é™„åŠ ä»»åŠ¡æç¤ºã€‚

        gen_messages = self._sanitize_messages_for_llm(state["messages"])

        request = LlmRequest(
            messages=gen_messages,
            tools=None,  # Strict mode: provide NO tools so it must output text (code)
            user=state["user"],
            temperature=0.0,
            max_tokens=self.config.max_tokens,
            stream=self.config.stream_responses,
            system_prompt=state["system_prompt"]
            + f"\n\nTASK: {instruction}\nOutput executable SQL only. No markdown.",
        )

        print("Generated LlmRequest for generate_sql Generation:", gen_messages)

        for mw in self.llm_middlewares:
            request = await mw.before_llm_request(request)

        response: LlmResponse
        if self.config.stream_responses:
            accumulated_content = ""
            async for chunk in self.llm_service.stream_request(request):
                if chunk.content:
                    accumulated_content += chunk.content
            response = LlmResponse(content=accumulated_content)
        else:
            response = await self.llm_service.send_request(request)

        for mw in self.llm_middlewares:
            response = await mw.after_llm_response(request, response)

        generated_sql = response.content
        if generated_sql:
            generated_sql = generated_sql.replace("```sql", "").replace("```", "").strip()
            if generated_sql.lower().startswith("generated sql:"):
                generated_sql = generated_sql[len("generated sql:"):].strip()

        state["messages"].append(LlmMessage(
            role="assistant", content=f"Generated SQL: {generated_sql}"))

        # æ›´æ–°ä»»åŠ¡å’ŒçŠ¶æ€å¡ç‰‡
        # await ui_queue.put(UiComponent(rich_component=card.set_status("success", "SQL Generated")))
        # User prefers specific SQL as a Card, not just a status update
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="completed")))

        if generated_sql:
            # Use CardComponent for the SQL as requested
            sql_card = CardComponent(
                title="Generated SQL",
                content=generated_sql,
                icon="ğŸ“",
                status="success",
                markdown=False  # No markdown packaging
            )
            await ui_queue.put(UiComponent(
                rich_component=sql_card,
                simple_component=SimpleTextComponent(text=generated_sql)
            ))

        return {
            "generated_sql": generated_sql,
            "tool_iterations": state["tool_iterations"] + 1
        }

    async def _node_execute_sql(self, state: AgentState) -> PartialAgentState:
        """æ‰§è¡Œå·²ç”Ÿæˆçš„ SQLã€‚"""
        ui_queue = state.get("ui_queue")
        generated_sql = state.get("generated_sql")
        context = state.get("tool_context")
        conversation = state.get("conversation")

        # æŸ¥æ‰¾ execute_current_sql çš„å·¥å…·è°ƒç”¨ IDï¼ˆç”¨äºé”™è¯¯å“åº”ï¼‰
        target_tool_id = None
        other_tool_ids = []
        response = state.get("llm_response")
        if response and response.tool_calls:
            for tc in response.tool_calls:
                if tc.name == "execute_current_sql":
                    target_tool_id = tc.id
                else:
                    other_tool_ids.append(tc.id)

        if not generated_sql:
            if target_tool_id:
                state["messages"].append(LlmMessage(
                    role="tool", content="Error: No SQL has been generated yet.", tool_call_id=target_tool_id))
            for ot_id in other_tool_ids:
                state["messages"].append(LlmMessage(
                    role="tool", content="Ignored.", tool_call_id=ot_id))
            return {}

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working", message="Executing SQL", detail="Running query..."
            )
        ))

        # æ·»åŠ ä»»åŠ¡å’ŒçŠ¶æ€å¡ç‰‡
        task = Task(title="Execute SQL", description="Running SQL query", status="in_progress")
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.add_task(task)))

        # æŸ¥æ‰¾çœŸå®çš„ RunSqlTool
        sql_tool = await self.tool_registry.get_tool("run_sql")
        if not sql_tool:
            if target_tool_id:
                state["messages"].append(LlmMessage(
                    role="tool", content="Error: 'run_sql' tool is not configured.", tool_call_id=target_tool_id))
            for ot_id in other_tool_ids:
                state["messages"].append(LlmMessage(
                    role="tool", content="Ignored.", tool_call_id=ot_id))
            
            # Update UI on failure
            # await ui_queue.put(UiComponent(rich_component=card.set_status("error", "Tool not configured")))
            await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="failed")))
            return {}

        try:
            # åŸºç¡€æ‰§è¡Œ
            # ä¾èµ–å·¥å…·å®ä¾‹çš„ execute æ–¹æ³•
            # éœ€è¦æ„é€ å·¥å…·æ‰€éœ€çš„å‚æ•°å¯¹è±¡
            args_model = sql_tool.get_args_schema()
            tool_args = args_model(sql=generated_sql)

            # ç›´æ¥è°ƒç”¨å·¥å…·ä»¥é¿å…æ¨¡å¼æ ¡éªŒçš„é¢å¤–å¼€é”€/ä¸åŒ¹é…
            result = await sql_tool.execute(context, tool_args)

            # æ›´æ–°ä»»åŠ¡å’ŒçŠ¶æ€å¡ç‰‡
            status = "success" if result.success else "error"
            description = "Query executed successfully" if result.success else f"Error: {result.error}"
            
            # å¦‚æœæ˜¯é”™è¯¯ï¼Œå¯èƒ½éœ€è¦æ›´è¯¦ç»†çš„æè¿°
            if not result.success:
                 # Check if the result has a UI component that is a Notification
                if result.ui_component and result.ui_component.rich_component and hasattr(result.ui_component.rich_component, "message"):
                    description = result.ui_component.rich_component.message

            # await ui_queue.put(UiComponent(rich_component=card.set_status(status, description)))
            await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="completed" if result.success else "failed")))

        except Exception as e:
            logger.error(f"SQL Execution failed: {e}")
            state["messages"].append(LlmMessage(role="system", content=f"SQL Execution Error: {e}"))
            
            # Update UI on exception
            # await ui_queue.put(UiComponent(rich_component=card.set_status("error", f"Exception: {str(e)}")))
            await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="failed")))
            return {}

        # å°†ç»“æœå†™å…¥ä¼šè¯
        # ä¸Šæ–‡å·²è·å¾—ç›¸å…³ ID

        conversation.add_message(Message(
            role="tool",
            content=result.result_for_llm if result.success else f"Error: {result.error}",
            tool_call_id=target_tool_id or "unknown"
        ))

        # å°†ç»“æœåŠ å…¥ä¸Šä¸‹æ–‡æ¶ˆæ¯
        state["messages"].append(LlmMessage(
            role="tool",
            content=result.result_for_llm if result.success else f"Error: {result.error}",
            tool_call_id=target_tool_id or "unknown"
        ))

        # å¤„ç†å…¶ä»–å·¥å…·è°ƒç”¨
        for ot_id in other_tool_ids:
            state["messages"].append(LlmMessage(
                role="tool", content="Tool call ignored in this step.", tool_call_id=ot_id))

        if result.ui_component:
            await ui_queue.put(result.ui_component)

        return {
            "sql_result": result.result_for_llm,
            "tool_iterations": state["tool_iterations"] + 1
        }

    async def _node_save_memory(self, state: AgentState) -> PartialAgentState:
        """
        è®°å¿†ä¿å­˜èŠ‚ç‚¹ï¼š
        å¦‚æœ SQL æ‰§è¡ŒæˆåŠŸï¼Œå°è¯•å°†è¿™æ¬¡æˆåŠŸçš„ (Question, SQL) å¯¹ä¿å­˜åˆ° AgentMemoryã€‚
        """
        ui_queue = state["ui_queue"]
        context = state["tool_context"]
        message = state["message"]
        generated_sql = state.get("generated_sql")
        sql_result = state.get("sql_result")

        # æ£€æŸ¥å‰ç½®æ¡ä»¶ï¼šå¿…é¡»æœ‰ç”Ÿæˆçš„SQLï¼Œä¸”æ‰§è¡ŒæˆåŠŸï¼ˆç»“æœä¸å«Errorï¼‰
        if not generated_sql or not sql_result or "Error" in sql_result:
            return {}

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è®°å¿†åŠŸèƒ½
        if not self.agent_memory:
            return {}

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†ä¿å­˜å·¥å…·
        save_tool = await self.tool_registry.get_tool("save_question_tool_args")
        if not save_tool:
            return {}
            
        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working", message="Saving Memory", detail="Learning from success..."
            )
        ))

        # æ·»åŠ ä»»åŠ¡
        task = Task(title="Save Memory", description="Saving successful query pattern", status="in_progress")
        await ui_queue.put(UiComponent(rich_component=TaskTrackerUpdateComponent.add_task(task)))

        try:
            # æ‰§è¡Œä¿å­˜
            args_model = save_tool.get_args_schema()
            tool_args = args_model(
                question=message,
                tool_name="run_sql",
                args={"sql": generated_sql}
            )
            
            result = await save_tool.execute(context, tool_args)
            
            await ui_queue.put(UiComponent(
                rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="completed", detail="Pattern saved")
            ))
            
            if result.ui_component:
                await ui_queue.put(result.ui_component)
                
        except Exception as e:
            logger.error(f"Memory save failed: {e}")
            await ui_queue.put(UiComponent(
                rich_component=TaskTrackerUpdateComponent.update_task(task.id, status="failed", detail=str(e))
            ))

        return {}

    async def _node_think(self, state: AgentState) -> PartialAgentState:
        """ä½¿ç”¨è™šæ‹Ÿå·¥å…·æ‰§è¡Œä¸€æ¬¡ LLM è¯·æ±‚ã€‚"""

        # 1. å®šä¹‰è™šæ‹Ÿå·¥å…·ï¼ˆæ›´æ–°ç‰ˆï¼‰
        virtual_tools = [
            ToolSchema(
                name="query_schema_metadata",
                description="CRITICAL: Use this tool FIRST to retreive the database schema (tables/columns) before generating any SQL. Execute invalid SQLs like `SELECT ... FROM sqlite_master` to find tables.",
                parameters={
                    "type": "object",
                    "properties": {
                        "sql": {"type": "string", "description": "The SQL query to inspect schema (e.g. 'SELECT name, sql FROM sqlite_master WHERE type=\"table\"')"}
                    },
                    "required": ["sql"]
                }
            ),
            ToolSchema(
                name="generate_sql",
                description="Generate a business logic SQL query based on the schema and user question.",
                parameters={
                    "type": "object", "properties": {"instruction": {"type": "string"}}, "required": ["instruction"]
                }
            ),
            ToolSchema(
                name="execute_current_sql",
                description="Execute the currently generated SQL query.",
                parameters={
                    "type": "object", "properties": {}, "required": []
                }
            )
        ]

        # 2. è¿‡æ»¤çœŸå®å·¥å…·
        real_tools = state.get("tool_schemas", [])
        filtered_tools = [t for t in real_tools if t.name != "run_sql"]

        # 3. åˆå¹¶å·¥å…·æ¸…å•
        available_tools = filtered_tools + virtual_tools

        # Sanitize working messages as well
        think_messages = self._sanitize_messages_for_llm(state["messages"])

        request = LlmRequest(
            messages=think_messages,
            tools=available_tools,
            user=state["user"],
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            stream=self.config.stream_responses,
            system_prompt=state["system_prompt"],
        )

        for mw in self.llm_middlewares:
            request = await mw.before_llm_request(request)

        response: LlmResponse
        if self.config.stream_responses:
            accumulated_content = ""
            accumulated_tool_calls = []
            async for chunk in self.llm_service.stream_request(request):
                if chunk.content:
                    accumulated_content += chunk.content
                if chunk.tool_calls:
                    accumulated_tool_calls.extend(chunk.tool_calls)
            response = LlmResponse(
                content=accumulated_content if accumulated_content else None,
                tool_calls=accumulated_tool_calls if accumulated_tool_calls else None
            )
        else:
            response = await self.llm_service.send_request(request)

        for mw in self.llm_middlewares:
            response = await mw.after_llm_response(request, response)

        # å§‹ç»ˆå°†åŠ©æ‰‹æ¶ˆæ¯è¿½åŠ åˆ°çŠ¶æ€ä¸­ï¼Œå³ä½¿åªæœ‰å·¥å…·è°ƒç”¨
        assistant_msg = LlmMessage(
            role="assistant",
            content=response.content or "",  # å³ä½¿ä¸º None ä¹Ÿä¿è¯ä¸ºå­—ç¬¦ä¸²
            tool_calls=response.tool_calls
        )
        state["messages"].append(assistant_msg)

        # åŒæ­¥å†™å…¥ä¼šè¯å¯¹è±¡
        state["conversation"].add_message(Message(
            role="assistant",
            content=response.content or "",
            tool_calls=response.tool_calls
        ))

        if response.content:
            ui_queue = state["ui_queue"]
            await ui_queue.put(UiComponent(
                rich_component=RichTextComponent(content=response.content, markdown=True),
                simple_component=SimpleTextComponent(text=response.content)
            ))

        return {"llm_response": response}

    async def _node_execute_tools(self, state: AgentState) -> PartialAgentState:
        """æ‰§è¡Œ LLM å“åº”ä¸­è¯·æ±‚çš„å·¥å…·ã€‚"""
        response = state["llm_response"]
        conversation = state["conversation"]
        ui_queue = state["ui_queue"]
        user = state["user"]
        context = state["tool_context"]

        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°ä¼šè¯
        assistant_msg = Message(
            role="assistant",
            content=response.content or "",
            tool_calls=response.tool_calls
        )
        conversation.add_message(assistant_msg)

        # è¾“å‡ºæ–‡æœ¬å†…å®¹
        if response.content:
            await ui_queue.put(UiComponent(
                rich_component=RichTextComponent(content=response.content, markdown=True),
                simple_component=SimpleTextComponent(text=response.content)
            ))

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="working",
                message="Executing tools...",
                detail=f"Running {len(response.tool_calls or [])} tools"
            )
        ))

        tool_results_data = []
        for tool_call in (response.tool_calls or []):
            # ä»»åŠ¡ UI
            tool_task = Task(
                title=f"Execute {tool_call.name}",
                description="Running tool...",
                status="in_progress"
            )
            await ui_queue.put(UiComponent(
                rich_component=TaskTrackerUpdateComponent.add_task(tool_task)
            ))

            # # çŠ¶æ€å¡ç‰‡ UI
            # card = StatusCardComponent(
            #     title=f"Executing {tool_call.name}",
            #     status="running",
            #     icon="âš™ï¸",
            #     metadata=tool_call.arguments
            # )
            # await ui_queue.put(UiComponent(rich_component=card))

            # é’©å­ï¼šå·¥å…·æ‰§è¡Œå‰
            tool = await self.tool_registry.get_tool(tool_call.name)
            if tool:
                for hook in self.lifecycle_hooks:
                    await hook.before_tool(tool, context)

            # æ‰§è¡Œ
            result = await self.tool_registry.execute(tool_call, context)

            # é’©å­ï¼šå·¥å…·æ‰§è¡Œå
            for hook in self.lifecycle_hooks:
                modified = await hook.after_tool(result)
                if modified:
                    result = modified

            # ä½¿ç”¨ç»“æœæ›´æ–° UI
            status = "success" if result.success else "error"
            await ui_queue.put(UiComponent(
                rich_component=card.set_status(status, result.result_for_llm)
            ))
            await ui_queue.put(UiComponent(
                rich_component=TaskTrackerUpdateComponent.update_task(
                    tool_task.id, status="completed"
                )
            ))

            if result.ui_component:
                await ui_queue.put(result.ui_component)

            tool_results_data.append({
                "tool_call_id": tool_call.id,
                "content": result.result_for_llm if result.success else (result.error or "Failed")
            })

        # å°†å·¥å…·æ¶ˆæ¯å†™å…¥ä¼šè¯
        for res in tool_results_data:
            conversation.add_message(Message(
                role="tool",
                content=res["content"],
                tool_call_id=res["tool_call_id"]
            ))

        # ä¸ºä¸‹ä¸€è½® LLM è¯·æ±‚é‡å»ºæ¶ˆæ¯åˆ—è¡¨
        # å®é™…å®ç°ä¸­å¯ç›´æ¥è¿½åŠ åˆ° state["messages"]ï¼›
        # ä½†æ­¤å¤„ä»¥ conversation.messages ä¸ºå‡†ï¼Œå¯èƒ½éœ€è¦é‡æ–°è½¬æ¢æˆ–è¿½åŠ ã€‚
        # ä¸ºç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ²¿ç”¨å‡†å¤‡ä¸Šä¸‹æ–‡æˆ–å¢é‡è¿½åŠ é€»è¾‘ã€‚

        # å¢é‡è¿½åŠ åˆ° LlmMessages
        new_messages = state["messages"][:]
        
        # _node_think å·²ç»å°† assistant_msg è¿½åŠ åˆ°äº† state["messages"] ä¸­
        # å¦‚æœ state["messages"] å·²ç»åŒ…å«æœ€æ–°çš„ assistant æ¶ˆæ¯ï¼Œåˆ™æ— éœ€å†æ¬¡è¿½åŠ 
        # ç®€å•åˆ¤æ–­ï¼šå¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ assistant ä¸” tool_calls ä¸å½“å‰ response ä¸€è‡´ï¼Œåˆ™è®¤ä¸ºæ˜¯åŒä¸€æ¡
        should_append_assistant = True
        if new_messages:
            last_msg = new_messages[-1]
            if (last_msg.role == "assistant" and 
                last_msg.tool_calls == response.tool_calls):
                should_append_assistant = False
        
        if should_append_assistant:
            new_messages.append(LlmMessage(
                role="assistant",
                content=response.content or "",
                tool_calls=response.tool_calls
            ))
            
        for res in tool_results_data:
            new_messages.append(LlmMessage(
                role="tool",
                content=res["content"],
                tool_call_id=res["tool_call_id"]
            ))

        return {
            "tool_iterations": state["tool_iterations"] + 1,
            "messages": new_messages
        }

    async def _node_finalize(self, state: AgentState) -> PartialAgentState:
        """ä¿å­˜ä¼šè¯ã€è§¦å‘é’©å­å¹¶æ”¶å°¾ã€‚"""
        if state.get("should_stop"):
            return {}

        conversation = state["conversation"]
        ui_queue = state["ui_queue"]

        # _node_think å’Œå…¶ä»–èŠ‚ç‚¹å·²ç»è´Ÿè´£äº†æ¶ˆæ¯çš„æ·»åŠ å’Œå†…å®¹çš„ UI æ¨é€
        # æ­¤å¤„ä¸»è¦è´Ÿè´£æ”¶å°¾å·¥ä½œï¼ˆçŠ¶æ€æ ã€è¾“å…¥æ¡†é‡ç½®ç­‰ï¼‰

        await ui_queue.put(UiComponent(
            rich_component=StatusBarUpdateComponent(
                status="idle", message="Response complete", detail="Ready"
            )
        ))
        await ui_queue.put(UiComponent(
            rich_component=ChatInputUpdateComponent(
                placeholder="Ask a follow-up...", disabled=False
            )
        ))

        if self.config.auto_save_conversations:
            await self.conversation_store.update_conversation(conversation)

        for hook in self.lifecycle_hooks:
            await hook.after_message(conversation)

        return {"is_complete": True}

    # --- è·¯ç”± ---

    def _router_check_stop(self, state: AgentState) -> Literal["stop", "continue"]:
        return "stop" if state.get("should_stop") else "continue"

    def _router_analyze_response(self, state: AgentState) -> Literal["tools", "done", "get_schema", "generate_sql", "execute_sql"]:
        response = state["llm_response"]

        if response and response.is_tool_call():
            # æ£€æŸ¥è™šæ‹Ÿå·¥å…·
            for tool_call in response.tool_calls:
                if tool_call.name == "query_schema_metadata":
                    return "get_schema"
                if tool_call.name == "generate_sql":
                    return "generate_sql"
                if tool_call.name == "execute_current_sql":
                    return "execute_sql"
            return "tools"
        return "done"

    def _router_check_limit(self, state: AgentState) -> Literal["continue", "stop"]:
        if state["tool_iterations"] < self.config.max_tool_iterations:
            return "continue"

        # è¾¾åˆ°å·¥å…·è¿­ä»£ä¸Šé™çš„é€»è¾‘å¯åœ¨æ­¤æ·»åŠ ï¼ˆæ—¥å¿—ã€è­¦å‘Š UIï¼‰
        return "stop"
